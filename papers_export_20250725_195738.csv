id,arxiv_id,title,abstract,authors,publication_date,categories,paper_type,evidence_strength,novelty_score,practical_applicability,analysis_status,analysis_confidence,extraction_version,content_generated,content_approved,ingestion_source,created_at,updated_at
43bfc09d-d3d0-4f97-aa96-53006aa1f4a8,2507.00535v1,Rethinking Group Recommender Systems in the Era of Generative AI: From   One-Shot Recommendations to Agentic Group Decision Support,"More than twenty-five years ago, first ideas were developed on how to design a system that can provide recommendations to groups of users instead of individual users. Since then, a rich variety of algorithmic proposals were published, e.g., on how to acquire individual preferences, how to aggregate them, and how to generate recommendations for groups of users. However, despite the rich literature on the topic, barely any examples of real-world group recommender systems can be found. This lets us question common assumptions in academic research, in particular regarding communication processes in a group and how recommendation-supported decisions are made. In this essay, we argue that these common assumptions and corresponding system designs often may not match the needs or expectations of users. We thus call for a reorientation in this research area, leveraging the capabilities of modern Generative AI assistants like ChatGPT. Specifically, as one promising future direction, we envision group recommender systems to be systems where human group members interact in a chat and an AI-based group recommendation agent assists the decision-making process in an agentic way. Ultimately, this shall lead to a more natural group decision-making environment and finally to wider adoption of group recommendation systems in practice.","Author(name='Author(name=""Author(name=\'Dietmar Jannach\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None); Author(name='Author(name=""Author(name=\'Amra DeliÄ‡\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None); Author(name='Author(name=""Author(name=\'Francesco Ricci\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None); Author(name='Author(name=""Author(name=\'Markus Zanker\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None)",2025-07-01,cs.IR; cs.AI,position_paper,theoretical,,high,failed,0.44,1,False,False,arxiv_api,2025-07-25T18:06:40.958593,2025-07-25T18:29:55.723806
ef8bf9ed-e622-487c-97d1-01714a8ed2bd,2001.08361v1,Scaling Laws for Neural Language Models,"We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.","Author(name=""Author(name='Jared Kaplan', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Sam McCandlish', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Tom Henighan', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Tom B. Brown', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Benjamin Chess', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Rewon Child', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Scott Gray', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Alec Radford', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Jeffrey Wu', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Dario Amodei', affiliation=None, email=None)"", affiliation=None, email=None)",2020-01-23,cs.LG; stat.ML,conceptual_framework,experimental,,high,manual_review,0.78,1,False,False,arxiv_api,2025-07-25T16:53:52.161087,2025-07-25T16:58:55.259304
a21464d6-f1e9-4b7e-9b63-fe78c774918d,1706.03762v7,Attention Is All You Need,"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","Author(name=""Author(name='Ashish Vaswani', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Noam Shazeer', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Niki Parmar', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Jakob Uszkoreit', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Llion Jones', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Aidan N. Gomez', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Lukasz Kaiser', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Illia Polosukhin', affiliation=None, email=None)"", affiliation=None, email=None)",2017-06-12,cs.CL; cs.LG,conceptual_framework,experimental,,medium,manual_review,0.67,1,False,False,arxiv_api,2025-07-25T13:51:34.815540,2025-07-25T13:51:40.189127
7c830db3-6c56-42e3-8085-6339ec459844,2412.14161v2,TheAgentCompany: Benchmarking LLM Agents on Consequential Real World   Tasks,"We interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. But how performant are AI agents at accelerating or even autonomously performing work-related tasks? The answer to this question has important implications both for industry looking to adopt AI into their workflows and for economic policy to understand the effects that adoption of AI may have on the labor market. To measure the progress of these LLM agents' performance on performing real-world professional tasks, in this paper we introduce TheAgentCompany, an extensible benchmark for evaluating AI agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers. We build a self-contained environment with internal web sites and data that mimics a small software company environment, and create a variety of tasks that may be performed by workers in such a company. We test baseline agents powered by both closed API-based and open-weights language models (LMs), and find that the most competitive agent can complete 30% of tasks autonomously. This paints a nuanced picture on task automation with LM agents--in a setting simulating a real workplace, a good portion of simpler tasks could be solved autonomously, but more difficult long-horizon tasks are still beyond the reach of current systems. We release code, data, environment, and experiments on https://the-agent-company.com.","Author(name=""Author(name='Frank F. Xu', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Yufan Song', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Boxuan Li', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Yuxuan Tang', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Kritanjali Jain', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Mengxue Bao', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Zora Z. Wang', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Xuhui Zhou', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Zhitong Guo', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Murong Cao', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Mingyang Yang', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Hao Yang Lu', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Amaad Martin', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Zhe Su', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Leander Maben', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Raj Mehta', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Wayne Chi', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Lawrence Jang', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Yiqing Xie', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Shuyan Zhou', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Graham Neubig', affiliation=None, email=None)"", affiliation=None, email=None)",2024-12-18,cs.CL,benchmark_comparison,experimental,,high,manual_review,0.79,1,False,False,arxiv_api,2025-07-25T13:22:33.740754,2025-07-25T13:47:36.189512
053dc3e0-b032-4a98-b76c-042406c6eca7,2504.17248v1,How Jungian Cognitive Functions Explain MBTI Type Prevalence in Computer   Industry Careers,"This study investigates the relationship between Carl Jung's cognitive functions and success in computer industry careers by analyzing the distribution of Myers-Briggs Type Indicator (MBTI) types among professionals in the field. Building on Carl Jung's theory of psychological types, which categorizes human cognition into four primary functions, Sensing, Intuition, Thinking, and Feeling, this study investigates how these functions, when combined with the attitudes of Extraversion and Introversion, influence personality types and career choices in the tech sector. Through a comprehensive analysis of data from 30 studies spanning multiple countries and decades, encompassing 18,264 individuals in computer-related professions, we identified the most prevalent cognitive functions and their combinations. After normalizing the data against general population distributions, our findings showed that individual Jungian functions (Te, Ni, Ti, Ne), dual function combinations (Ni-Te, Ti-Ne, Si-Te, Ni-Fe), and MBTI types (INTJ, ENTJ, INTP, ENTP, ISTJ, INFJ, ESTJ, ESTP) had significantly higher representation compared to general population norms. The paper addresses gaps in the existing literature by providing a more nuanced understanding of how cognitive functions impact job performance and team dynamics, offering insights for career guidance, team composition, and professional development in the computer industry, and a deeper understanding of how cognitive preferences influence career success in technology-related fields.","Author(name='Author(name=""Author(name=\'Arya VarastehNezhad\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None); Author(name='Author(name=""Author(name=\'Behnam Agahi\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None); Author(name='Author(name=""Author(name=\'Soroush Elyasi\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None); Author(name='Author(name=""Author(name=\'Reza Tavasoli\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None); Author(name='Author(name=""Author(name=\'Hamed Farbeh\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None)",2025-04-24,cs.CY,empirical_study,observational,,high,completed,0.79,1,False,False,arxiv_api,2025-07-25T13:22:12.703273,2025-07-25T20:33:41.370125
83d19c26-8b7f-41f6-bb63-c4eeaee0ff98,2506.08872v1,Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI   Assistant for Essay Writing Task,"This study explores the neural and behavioral consequences of LLM-assisted essay writing. Participants were divided into three groups: LLM, Search Engine, and Brain-only (no tools). Each completed three sessions under the same condition. In a fourth session, LLM users were reassigned to Brain-only group (LLM-to-Brain), and Brain-only users were reassigned to LLM condition (Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18 completing session 4. We used electroencephalography (EEG) to assess cognitive load during essay writing, and analyzed essays using NLP, as well as scoring essays with the help from human teachers and an AI judge. Across groups, NERs, n-gram patterns, and topic ontology showed within-group homogeneity. EEG revealed significant differences in brain connectivity: Brain-only participants exhibited the strongest, most distributed networks; Search Engine users showed moderate engagement; and LLM users displayed the weakest connectivity. Cognitive activity scaled down in relation to external tool use. In session 4, LLM-to-Brain participants showed reduced alpha and beta connectivity, indicating under-engagement. Brain-to-LLM users exhibited higher memory recall and activation of occipito-parietal and prefrontal areas, similar to Search Engine users. Self-reported ownership of essays was the lowest in the LLM group and the highest in the Brain-only group. LLM users also struggled to accurately quote their own work. While LLMs offer immediate convenience, our findings highlight potential cognitive costs. Over four months, LLM users consistently underperformed at neural, linguistic, and behavioral levels. These results raise concerns about the long-term educational implications of LLM reliance and underscore the need for deeper inquiry into AI's role in learning.","Author(name='Author(name=""Author(name=\'Nataliya Kosmyna\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None); Author(name='Author(name=""Author(name=\'Eugene Hauptmann\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None); Author(name='Author(name=""Author(name=\'Ye Tong Yuan\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None); Author(name='Author(name=""Author(name=\'Jessica Situ\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None); Author(name='Author(name=""Author(name=\'Xian-Hao Liao\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None); Author(name='Author(name=""Author(name=\'Ashly Vivian Beresnitzky\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None); Author(name='Author(name=""Author(name=\'Iris Braunstein\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None); Author(name='Author(name=""Author(name=\'Pattie Maes\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None)",2025-06-10,cs.AI,empirical_study,observational,,medium,completed,0.58,1,False,False,arxiv_api,2025-07-25T13:21:47.994769,2025-07-25T20:33:41.374625
3c9141c1-6e63-482a-a454-202570262c78,2401.02777v2,From LLM to Conversational Agent: A Memory Enhanced Architecture with   Fine-Tuning of Large Language Models,"This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirroring human short-term and long-term memory, to maintain context and continuity in conversations. It entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase. This approach appears to enhance agent controllability and adaptability in complex, multi-turn dialogues. Our preliminary evaluations in a real estate sales context suggest that RAISE has some advantages over traditional agents, indicating its potential for broader applications. This work contributes to the AI field by providing a robust framework for developing more context-aware and versatile conversational agents.","Author(name='Author(name=""Author(name=\'Na Liu\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None); Author(name='Author(name=""Author(name=\'Liangyu Chen\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None); Author(name='Author(name=""Author(name=\'Xiaoyu Tian\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None); Author(name='Author(name=""Author(name=\'Wei Zou\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None); Author(name='Author(name=""Author(name=\'Kaijiang Chen\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None); Author(name='Author(name=""Author(name=\'Ming Cui\', affiliation=None, email=None)"", affiliation=None, email=None)', affiliation=None, email=None)",2024-01-05,cs.CL; cs.AI,conceptual_framework,theoretical,,medium,completed,0.67,1,False,False,arxiv_api,2025-07-25T13:21:14.117280,2025-07-25T20:33:41.376865
600b907e-f8b3-405c-acf6-3f27e5f280d7,2504.00338v1,Agentic Multimodal AI for Hyperpersonalized B2B and B2C Advertising in   Competitive Markets: An AI-Driven Competitive Advertising Framework,"The growing use of foundation models (FMs) in real-world applications demands adaptive, reliable, and efficient strategies for dynamic markets. In the chemical industry, AI-discovered materials drive innovation, but commercial success hinges on market adoption, requiring FM-driven advertising frameworks that operate in-the-wild. We present a multilingual, multimodal AI framework for autonomous, hyper-personalized advertising in B2B and B2C markets. By integrating retrieval-augmented generation (RAG), multimodal reasoning, and adaptive persona-based targeting, our system generates culturally relevant, market-aware ads tailored to shifting consumer behaviors and competition. Validation combines real-world product experiments with a Simulated Humanistic Colony of Agents to model consumer personas, optimize strategies at scale, and ensure privacy compliance. Synthetic experiments mirror real-world scenarios, enabling cost-effective testing of ad strategies without risky A/B tests. Combining structured retrieval-augmented reasoning with in-context learning (ICL), the framework boosts engagement, prevents market cannibalization, and maximizes ROAS. This work bridges AI-driven innovation and market adoption, advancing multimodal FM deployment for high-stakes decision-making in commercial marketing.","Author(name=""Author(name='Sakhinana Sagar Srinivas', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Akash Das', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Shivam Gupta', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Venkataramana Runkana', affiliation=None, email=None)"", affiliation=None, email=None)",2025-04-01,cs.LG; cs.AI; cs.MA; cs.SI,conceptual_framework,theoretical,,high,completed,0.82,1,False,False,arxiv_api,2025-07-25T13:18:15.557387,2025-07-25T13:49:25.046447
86972654-b802-46f5-ab83-86c178bb955a,2503.21460v1,"Large Language Model Agent: A Survey on Methodology, Applications and   Challenges","The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers.","Author(name=""Author(name='Junyu Luo', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Weizhi Zhang', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Ye Yuan', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Yusheng Zhao', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Junwei Yang', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Yiyang Gu', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Bohan Wu', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Binqi Chen', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Ziyue Qiao', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Qingqing Long', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Rongcheng Tu', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Xiao Luo', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Wei Ju', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Zhiping Xiao', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Yifan Wang', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Meng Xiao', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Chenwu Liu', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Jingyang Yuan', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Shichang Zhang', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Yiqiao Jin', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Fan Zhang', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Xian Wu', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Hanqing Zhao', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Dacheng Tao', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Philip S. Yu', affiliation=None, email=None)"", affiliation=None, email=None); Author(name=""Author(name='Ming Zhang', affiliation=None, email=None)"", affiliation=None, email=None)",2025-03-27,cs.CL,position_paper,theoretical,,high,completed,0.93,1,False,False,arxiv_api,2025-07-25T13:15:42.932700,2025-07-25T13:49:25.051707
